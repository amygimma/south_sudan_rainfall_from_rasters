}
head(SubsetGAMDF)
write.csv(SubsetGAMDF,file="C:\\Users\\Sev\\Documents\\Paper 1_R files\\SubsetGAMDF.csv")
SubsetDF <- subset(SubsetDF, survey_code!="ACF_191", survey_code!="FSNAU_12", survey_code!="FSNAU_25", survey_code!="FSNAU_27")
SubsetDF <- subset(SubsetDF, survey_code!="ACF_191" & survey_code!="FSNAU_12" & survey_code!="FSNAU_25" & survey_code!="FSNAU_27")
length(table(SubsetDF$survey_code))
# delete records with missing cluster
SubsetDF<-SubsetDF[!is.na(SubsetDF$cluster),]
length(table(SubsetDF$survey_code))
# one survey had all missing values for cluster
attach(SubsetDF)
SubsetGAMDF<- as.data.frame(matrix(NA, ncol=9, nrow=length(unique(SubsetDF$survey_code))))
names(SubsetGAMDF) <- c( "survey_code","ngam", "muacgam", "cat", "nwfhgam", "wfhgam","catwfh", "oedemape", "noedema")
length <- c()
ngam <- c()
nwfhgam <- c()
noedema<-c()
for (i in 1: length(unique(SubsetDF$survey_code)))
{
SubsetGAMDF[i,"survey_code"] <- unique(SubsetDF$survey_code)[i]
y<- subset(SubsetDF, survey_code == unique(SubsetDF[, "survey_code"])[i])
ngam[i] <- table(y$muac<125)[2]
length[i] <- length(na.omit(y$muac))
SubsetGAMDF[i,"muacgam"] <- ngam[i]/ length[i]*100
SubsetGAMDF[i,"ngam"] <- ngam[i]
if (SubsetGAMDF[i, "muacgam"] < 5) {SubsetGAMDF[i,"cat"] <- 1} else
if ((SubsetGAMDF[i, "muacgam"] >= 5) && (SubsetGAMDF[i, "muacgam"] < 10)) {SubsetGAMDF[i,"cat"] <- 2} else
if ((SubsetGAMDF[i, "muacgam"]>= 10) && (SubsetGAMDF[i, "muacgam"] < 15)) {SubsetGAMDF[i,"cat"] <-3} else
if (SubsetGAMDF[i, "muacgam"]>= 15) {SubsetGAMDF[i,"cat"] <-4}
nwfhgam [i] <-  table(y$X_zwfl< -2.00)[2]
SubsetGAMDF[i,"wfhgam"] <- nwfhgam [i]/length[i] *100
SubsetGAMDF[i,"nwfhgam"] <- nwfhgam [i]
if (SubsetGAMDF[i, "wfhgam"] < 5) {SubsetGAMDF[i,"catwfh"] <- 1} else
if ((SubsetGAMDF[i, "wfhgam"] >= 5) && (SubsetGAMDF[i, "wfhgam"] < 10)) {SubsetGAMDF[i,"catwfh"] <- 2} else
if ((SubsetGAMDF[i, "wfhgam"]>= 10) && (SubsetGAMDF[i, "wfhgam"] < 15)) {SubsetGAMDF[i,"catwfh"] <-3} else
if (SubsetGAMDF[i, "wfhgam"]>= 15) {SubsetGAMDF[i,"catwfh"] <-4}
noedema[i] <- table((y$oedema==1), useNA="no")[2]
if(all(y$oedema==0 | is.na(y$oedema))){
SubsetGAMDF[i, "oedemape"]<-0} else
{
SubsetGAMDF[i, "oedemape"]<- noedema[i]/ length[i]*100}
if(all(y$oedema==0 | is.na(y$oedema))){
SubsetGAMDF[i, "noedema"]<-0} else
{SubsetGAMDF[i, "noedema"] <- noedema[i]}
}
head(SubsetGAMDF)
for (i in 1: length(unique(SubsetDF$survey_code)))
{
SubsetGAMDF[i,"survey_code"] <- unique(SubsetDF$survey_code)[i]
y<- subset(SubsetDF, survey_code == unique(SubsetDF[, "survey_code"])[i])
if(all(y$oedema==0 | is.na(y$oedema))){
SubsetGAMDF[i, "gamuaco"]<-ngam[i]/ length[i]*100} else
{SubsetGAMDF[i,"gamuaco"] <- (ngam[i] + noedema[i])/ length[i]*100}
if(all(y$oedema==0 | is.na(y$oedema))){
SubsetGAMDF[i, "gamwfho"]<-nwfhgam[i]/ length[i]*100} else {
SubsetGAMDF[i,"gamwfho"] <- (nwfhgam [i] +noedema[i])/length[i] *100}
}
head(SubsetGAMDF)
nsam <- c()
length <- c()
nwfhsam <- c()
for (i in 1: length(unique(SubsetDF$survey_code)))
{
SubsetGAMDF[i,"survey_code"] <- unique(SubsetDF$survey_code)[i]
y<- subset(SubsetDF, survey_code == unique(SubsetDF[, "survey_code"])[i])
nsam[i] <- table(y$muac<115)[2]
length[i] <- length(na.omit(y$muac))
SubsetGAMDF[i,"muacsam"] <- nsam[i]/ length[i]*100
SubsetGAMDF[i,"nsam"] <- nsam[i]
nwfhsam [i] <-  table(y$X_zwfl< -3.00)[2]
SubsetGAMDF[i,"wfhsam"] <- nwfhsam [i]/length[i] *100
SubsetGAMDF[i,"nwfhsam"] <- nwfhsam [i]
}
head(SubsetGAMDF)
nhaz <- c()
for (i in 1: length(unique(SubsetDF$survey_code)))
{
SubsetGAMDF[i,"survey_code"] <- unique(SubsetDF$survey_code)[i]
y<- subset(SubsetDF, survey_code == unique(SubsetDF[, "survey_code"])[i])
nhaz [i] <-  table(y$X_zlen< -2.00)[2]
SubsetGAMDF[i,"haz"] <- nhaz [i]/length[i] *100
SubsetGAMDF[i,"nhaz"] <- nhaz [i]
if (SubsetGAMDF[i, "haz"] < 20) {SubsetGAMDF[i,"cathaz"] <- 1} else
if ((SubsetGAMDF[i, "haz"] >= 20) && (SubsetGAMDF[i, "haz"] < 30)) {SubsetGAMDF[i,"cathaz"] <- 2} else
if ((SubsetGAMDF[i, "haz"]>= 30) && (SubsetGAMDF[i, "haz"] < 40)) {SubsetGAMDF[i,"cathaz"] <-3} else
if (SubsetGAMDF[i, "haz"]>= 40) {SubsetGAMDF[i,"cathaz"] <-4}
}
OverlapDF <- read.csv("C:\\Users\\Sev\\Documents\\Paper 1_R files\\OverlapDF.csv",
stringsAsFactors=FALSE, header=T)
OverlapDF <- subset(OverlapDF, survey_code!="FSNAU_12" & survey_code!="FSNAU_25" & survey_code!="FSNAU_27")
for (i in 1: length(unique(OverlapDF$survey_code)))
{
y<- subset(OverlapDF, survey_code == unique(OverlapDF[, "survey_code"])[i])
SubsetGAMDF[i,"overlapmuac"]<- table(y$overlapmuac)[2]
SubsetGAMDF[i,"overlapwfh"]<- table(y$overlapwfh)[2]
SubsetGAMDF[i,"overlapmuacwfh"]<-table(y$overlapmuacwfh)[2]
}
head(SubsetGAMDF)
for (i in 1: length(unique(OverlapDF$survey_code)))
{
y<- subset(OverlapDF, survey_code == unique(OverlapDF[, "survey_code"])[i])
SubsetGAMDF[i,"region"]<- y$region[i]
SubsetGAMDF[i,"country"]<- y$country[i]
SubsetGAMDF[i,"residence"]<- y$residence[i]
SubsetGAMDF[i,"livelihood"]<- y$livelihood[i]
SubsetGAMDF[i,"date"]<- y$date[i]
}
head(SubsetGAMDF)
for (i in 1: length(unique(SubsetDF$survey_code)))
{
y<- subset(SubsetDF, survey_code == unique(SubsetDF[, "survey_code"])[i])
SubsetGAMDF[i,"total"]  <- length(na.omit(y$muac))
}
write.csv(SubsetGAMDF, file="C:\\Users\\Sev\\Documents\\Paper 1_R files\\SubsetGAMDF.csv")
head(SubsetGAMDF)
write.csv(SubsetGAMDF, file="C:\\Users\\Sev\\Documents\\Paper 1_R files\\SubsetGAMDF.csv")
write.csv(SubsetGAMDF, file="C:\Users\Sev\Documents\Paper 1_R files\SubsetGAMDF.csv")
write.csv(SubsetGAMDF, file="C:\Users\Sev\Documents\Paper 1_R files\SubsetGAMDF.csv")
write.csv(SubsetGAMDF, file="C:\\Users\\Sev\\Documents\\Paper 1_R files\\SubsetGAMDF.csv")
SubsetDF<-read.csv("C:\\Users\\Sev\\Documents\\Paper 1_R files\\889_.csv", stringsAsFactors=FALSE, header=T)
head(SubsetDF)
SubsetDF<-read.csv("C:\\Users\\Sev\\Documents\\Paper 1_R\\889_.csv", stringsAsFactors=FALSE, header=T)
head(SubsetDF)
table(sex, exclude=NULL)
table(SubsetDF$sex, exclude=NULL)
require(VennDiagram)
install.packages("extrafont")
require(extrafont)
# Wasting(muac)
draw.pairwise.venn(65680,3230,1233, ext.text=FALSE, fontfamily="Calibri")
install.packages("VennDiagram")
require(VennDiagram)
# Wasting(muac)
draw.pairwise.venn(65680,3230,1233, ext.text=FALSE, fontfamily="Calibri")
grid.newpage()
draw.pairwise.venn(65680,3230,1233, ext.text=TEXT, fontfamily="Calibri")
draw.pairwise.venn(65680,3230,1233, ext.text=FALSE, fontfamily="Calibri")
draw.pairwise.venn(65680,3230,1233, fontfamily="Calibri")
grid.newpage()
require(extrafont)
# Wasting(muac)
draw.pairwise.venn(65680,3230,1233, ext.text=FALSE, fontfamily="Calibri"), c("MUAC<125mm", "Edema"), ext.text=FALSE, cat.fontfamily="Calibri")
draw.pairwise.venn(65680,3230,1233, ext.text=FALSE, fontfamily="Calibri", c("MUAC<125mm", "Edema"), ext.text=FALSE, cat.fontfamily="Calibri")
draw.pairwise.venn(65680,3230,1233, ext.text=FALSE, fontfamily="Calibri", c("MUAC<125mm", "Edema"), ext.text=FALSE, cat.fontfamily="Calibri")
draw.pairwise.venn(65680,3230,1233, fontfamily="Calibri", c("MUAC<125mm", "Edema"), ext.text=FALSE, cat.fontfamily="Calibri")
# Wasting(muac)
draw.pairwise.venn(65680,3230,1233, fontfamily="Calibri", c("MUAC<125mm", "Edema"), cat.fontfamily="Calibri")
grid.newpage()
draw.pairwise.venn(65680,3230,1233, fontfamily="Calibri", c("MUAC<125mm", "Edema"), cat.fontfamily="Calibri")
grid.newpage()
draw.pairwise.venn(65680,3230,1233, fill=NULL)
grid.newpage()
draw.pairwise.venn(65680,3230,1233, fill=NULL)
install.packages("venneuler")
library(venneuler)
library(venneuler)
library("rJava", lib.loc="~/R/win-library/3.0")
install.packages("rJava")
library(venneuler)
require(rJava)
library(rJava)
install.packages("rJava")
library(rJava)
# Wasting(muac)
draw.pairwise.venn(65680,3230,1233, ext.text=FALSE)
grid.newpage()
draw.pairwise.venn(65680,3230,1233, ext.text=FALSE)
library(venneuler)
grid.newpage()
draw.pairwise.venn(65680,3230,1233, ext.text=FALSE, fontfamily="Calibri")
# Wasting(muac)
draw.pairwise.venn(65680,3230,1233, ext.text=FALSE, fontfamily="Calibri", 3)
require(VennDiagram)
require(extrafont)
draw.pairwise.venn(65680,3230,1233, ext.text=FALSE, fontfamily="Calibri", 3)
grid.newpage()
draw.pairwise.venn(65680,3230,1233, ext.text=FALSE, fontfamily="serif", 3)
grid.newpage()
draw.pairwise.venn(65680,3230,1233, ext.text=FALSE, fontfamily="serif")
require(VennDiagram)
require(extrafont)
draw.pairwise.venn(65680,3230,1233, ext.text=FALSE)
SubsetDF<-read.csv("H:\\My Documents\\Final_Databases\\Database.csv",
stringsAsFactors=FALSE, header=T)
SubsetDF<-read.csv("C:\\Users\\Sev\\Documents\\Final_Databases\\Database.csv",
stringsAsFactors=FALSE, header=T)
SubsetDF <- subset(SubsetDF, survey_code!="ACF_11")
write.csv(SubsetDF, file="C:\\Users\\Sev\\Documents\\ACF_11.csv")
SubsetDF <- subset(SubsetDF, survey_code!="UNICEF_3")
write.csv(SubsetDF, file="C:\\Users\\Sev\\Documents\\UNICEF_3.csv")
SubsetDF <- subset(SubsetDF, survey_code!="FSNAU_25")
write.csv(SubsetDF, file="C:\\Users\\Sev\\Documents\\FSNAU_25.csv")
SubsetDF <- subset(SubsetDF, survey_code!="Goal_2")
write.csv(SubsetDF, file="C:\\Users\\Sev\\Documents\\Goal_2.csv")
subsetDF <- subset(SubsetDF, survey_code!="ACF_11")
write.csv(SubsetDF, file="C:\\Users\\Sev\\Documents\\ACF_11.csv")
subsetDF <- subset(SubsetDF, survey_code!="UNICEF_3")
write.csv(SubsetDF, file="C:\\Users\\Sev\\Documents\\UNICEF_3.csv")
subsetDF <- subset(SubsetDF, survey_code!="ACF_11")
write.csv(subsetDF, file="C:\\Users\\Sev\\Documents\\ACF_11.csv")
subsetDF <- subset(SubsetDF, survey_code!="UNICEF_3")
write.csv(subsetDF, file="C:\\Users\\Sev\\Documents\\UNICEF_3.csv")
subsetDF <- subset(SubsetDF, survey_code!="FSNAU_25")
write.csv(subsetDF, file="C:\\Users\\Sev\\Documents\\FSNAU_25.csv")
subsetDF <- subset(SubsetDF, survey_code!="Goal_2")
write.csv(subsetDF, file="C:\\Users\\Sev\\Documents\\Goal_2.csv")
subsetDF <- subset(SubsetDF, survey_code=="ACF_11")
write.csv(subsetDF, file="C:\\Users\\Sev\\Documents\\ACF_11.csv")
subsetDF <- subset(SubsetDF, survey_code=="UNICEF_3")
write.csv(subsetDF, file="C:\\Users\\Sev\\Documents\\UNICEF_3.csv")
subsetDF <- subset(SubsetDF, survey_code=="FSNAU_25")
write.csv(subsetDF, file="C:\\Users\\Sev\\Documents\\FSNAU_25.csv")
subsetDF <- subset(SubsetDF, survey_code=="Goal_2")
write.csv(subsetDF, file="C:\\Users\\Sev\\Documents\\Goal_2.csv")
subsetDF <- subset(SubsetDF, survey_code=="UNICEF_3")
head(subset)
head(subsetDF)
subsetDF <- subset(SubsetDF, survey_code=="UNICEF_9")
head(subsetDF)
write.csv(subsetDF, file="C:\\Users\\Sev\\Documents\\UNICEF_9.csv")
SubsetDF<-read.csv("C:\\Users\\Sev\\Documents\\Final_Databases\\Database.csv",
stringsAsFactors=FALSE, header=T)
head(SubsetDF)
subsetDF <- subset(SubsetDF, survey_code=="ACF_11")
head(subsetDF)
shapiro.test(subsetDF$muac)
MainDF<-read.csv("C:\\Users\\Sev\\Documents\\Final_Databases\\Database.csv",
stringsAsFactors=FALSE, header=T)
NormalDF<- as.data.frame(matrix(NA, ncol=1, nrow=length(unique(MainDF$survey_code))))
names(NormalDF) <- c( "survey_code")
head(NormalDF)
for (i in 1: length(unique(MainDF[, "survey_code"])))
{
x <- subset(MainDF, survey_code == unique(MainDF[, "survey_code"])[i])
NormalityDF[i,"survey_code"] <- unique(MainDF$survey_code)[i]
NormalityDF[i,"shapiro"]<- shapiro.test(x[, "muac"])$p.value
NormalityDF[i,"W"]<-shapiro.test(x[, "muac"])$W
}
head(NormalDF)
for (i in 1: length(unique(MainDF[, "survey_code"])))
{
x <- subset(MainDF, survey_code == unique(MainDF[, "survey_code"])[i])
NormalDF[i,"survey_code"] <- unique(MainDF$survey_code)[i]
NormalDF[i,"shapiro"]<- shapiro.test(x[, "muac"])$p.value
NormalDF[i,"W"]<-shapiro.test(x[, "muac"])$W
}
head(NormalDF)
for (i in 1: length(unique(MainDF[, "survey_code"])))
{
x <- subset(MainDF, survey_code == unique(MainDF[, "survey_code"])[i])
NormalDF[i,"survey_code"] <- unique(MainDF$survey_code)[i]
NormalDF[i,"shapiro"]<- shapiro.test(x[, "muac"])$p.value
}
shapiro.test(subsetDF$muac)
W
subsetDF$muac$
W
subsetDF$muac[1]
shapiro.test(subsetDF[, "muac"])$W
shapiro.test(subsetDF[, "muac"])$p.value
shapiro.test(subsetDF[, "muac"])$w
shapiro.test(subsetDF[, "muac"])$wilk
shapiro.test(subsetDF[, "muac"])$1
shapiro.test(subsetDF[, "muac"])$[1]
shapiro.test(subsetDF[, "muac"])[1]
for (i in 1: length(unique(MainDF[, "survey_code"])))
{
x <- subset(MainDF, survey_code == unique(MainDF[, "survey_code"])[i])
NormalDF[i,"survey_code"] <- unique(MainDF$survey_code)[i]
NormalDF[i,"pvalue"]<- shapiro.test(x[, "muac"])$p.value
NormalDF[i,"W"]<-shapiro.test(x[, "muac"])[1]
}
head(NormalDF)
write.csv(NormalDF,file=""C:\\Users\\Sev\\Documents\\Normal\\NormalDF.csv")
write.csv(NormalDF,file=C:\\Users\\Sev\\Documents\\Normal\\NormalDF.csv")
write.csv(NormalDF,file="C:\\Users\\Sev\\Documents\\Normal\\NormalDF.csv")
install.packages("rjags")
install.packages("BEST")
sdDF<-read.csv("C:\\Users\\Sev\\Documents\\Bayesian approach\\sdDF.csv", na="0")
head(sdDF)
hist(sdDF$SDmuac, breaks = seq(0, 20, 0.2), main=" Distribution of MUAC SD", xlab="MUAC SD", ylab="Number of surveys")
require(MASS)
require(fitdistrplus)
plotdist(sdDF$SDmuac, histo=TRUE, demp=TRUE)
descdist(sdDF$SDmuac, boot=1000)
install.packages("fitdistrplus")
require(MASS)
require(fitdistrplus)
plotdist(sdDF$SDmuac, histo=TRUE, demp=TRUE)
descdist(sdDF$SDmuac, boot=1000)
plotdist(sdDF$SDmuac, histo=TRUE, demp=TRUE)
descdist(sdDF$SDmuac, boot=1000)
descdist(sdDF$SDmuac)
dist <- fitdist(sdDF$SDmuac, "logis", "mle")
summary(dist)
dist <- fitdist(sdDF$SDmuac, "logis")
summary(dist)
sdDF$SDmuac
install.packages("ggplot2")
require(ggplot2)
geom_density(sdDF$SDmuac)
ggplot(sdDF$SDmuac)
require(rjags)
require(BEST)
install.packages("ROCR")
install.packages(c("lubridate", "zoo"))
if(!require(tidverse) ) {install.packages("tidverse") ; library(tidverse) }
if(!require(Hmisc) ) {install.packages("Hmisc") ; library(Hmisc) }
I decided to go with Hmisc. There is a document in the imputation folder with other options: "J:/Mortality_estimation_crises/ImputationDealing with missing data.doc
# Hmisc
# Has two functions: impute() and aregImpute(). Treats categorical and non categorical variables the same way
# -	impute() function simply imputes missing value using od (mean, max, mean). It’s default is median.
# -	aregImpute() allows mean imputation using additive regression, bootstrapping, and predictive mean matching (pmm: for each observation in a variable with missing value, we find observation (from available values) with the closest predictive mean to that variable. The observed value from this “match” is then used as imputed value).
# 1.	It assumes linearity in the variables being predicted.
# 2.	Fisher’s optimum scoring method is used for predicting categorical variables.
# https://cran.r-project.org/web/packages/Hmisc/Hmisc.pdf
### Load necessary packages ###
rm(list=ls(all=TRUE) )
windowsFonts(Arial=windowsFont("Arial"))
# required packages (install first if not done already)
if(!require(gtools) ) {install.packages("gtools"); library(gtools) }
if(!require(lubridate) ) {install.packages("lubridate") ; library(lubridate) }
if(!require(ggplot2) ) {install.packages("ggplot2") ; library(ggplot2) }
if(!require(readxl) ) {install.packages("readxl") ; library(readxl) }
if(!require(reshape2) ) {install.packages("reshape2") ; library(reshape2) }
if(!require(zoo) ) {install.packages("zoo") ; library(zoo) }
if(!require(tidverse) ) {install.packages("tidverse") ; library(tidverse) }
if(!require(Hmisc) ) {install.packages("Hmisc") ; library(Hmisc) }
### Load any necessary naming frames and cleaned datasets ###
# MissForests, which is used by HMisc for pmm etc, does not like tibbles from tidyverse/dplyr so if these are used
# ...then the corresponding data needs to be reformatted as in the below chunk. You can ignore for base R.
# Also, this issue is supposedly fixed in a newer version of HMisc (1.4) that is used with newest R v3.6.1
dir <- "C:\Users\Sev\Documents\Nigeria_mortality estimates\Imputation"
market_df <- read_excel(
paste0(dir, "nga_market_prices.xlsx", sep=""))
###replace with data in the below.
cleaned_dat <- read_excel("cleaned_filename.xlsx", sep="") %>%
mutate((c(any_factor_vars_with_numbers))=(as.numeric(as.character(c(any_factor_vars_with_numbers)))))) %>%
droplevels() %>%
as.data.frame()
### Apply HMisc with pmm: for each observation in a variable with missing value, we find observation (from available values)
#with the closest predictive mean to that variable. The observed value from this “match” is then used as imputed value
form <-paste0('~', paste(c(your_imputation_covariates, 'column_to_be_imputed'), collapse=("+")
name_your_imputed_column <- aregImpute(as.formula(form), data=cleaned_dat, n.impute = 15, nk = 0, type='pmm' )
name_your_imputed_values <- name_your_imputed_column$imputed$column_to_be_imputed %>%
as.data.frame()
name_your_imputed_values_rowmean <- name_your_imputed_values %>%
mutate(row_mean = rowMeans(.[names(name_your_imputed_values)])) %>%
rownames_to_column()
write.csv(named_your_imputed_values_rowmean, "imputations_filename.csv", sep="")
cleaned_dat_imputed <- cleaned_dat %>%
rownames_to_column() %>%
left_join(imputed_values_rowmean) %>%
mutate(variable_multiple_imputation = coalesce(column_to_be_imputed,row_mean)) %>%
group_by(group1, group2) %>%
mutate(variable_mean_imputation = if_else(is.na(column_to_be_imputed), mean(column_to_be_imputed, na.rm=TRUE),
column_to_be_imputed)) %>%
ungroup()
### Run some checks that the imputation worked and return to tidy format as desired ###
names(cleaned_dat_imputed)
### Clean up individual imputations from HMisc from the analysis file (saved above in imputations_filename) ###
cleaned_dat_imputed <- dplyr::select(cleaned_dat_imputed, -starts_with("v"))
write.csv(cleaned_dat_imputed, "imputed_file_for_analysis.csv", sep="")
if(!require(dplyr) ) {install.packages("dplyr") ; library(dplyr) }
## Hmisc
# Assumes linearity in all variables being predicted. Fisherâs optimum scoring method is used for predicting categorical variables.
# install package and load library
install.packages("Hmisc")
install.packages("Rtools")
install.packages("tools")
install.packages("tools")
install.packages("tools")
install.packages("tools")
install.packages("tools")
install.packages("tools")
install.packages("tools")
install.packages("Rtools")
install.packages("pkgbuild") # pkgbuild is not available (for R version 3.5.0)
install.packages("devtools") # make sure you have the latest version from CRAN
library(devtools) # load package
devtools::install_github("r-lib/pkgbuild") # install updated version of pkgbuild from GitHub
.libPaths("C:/Users/Sev/Documents/R/win-library/4.2")
install.packages("pacman")
pacman::p_load(stringr, sf,RColorBrewer, raster, dplyr, rgdal, mapview, csv, openxlsx, purrr, testthat, microbenchmark, psych, update = FALSE)
.libPaths("C:/Users/Sev/Documents/R/win-library/4.2")
install.packages("pacman")
pacman::p_load(stringr, sf,RColorBrewer, raster, dplyr, rgdal, mapview, csv, openxlsx, purrr, testthat, microbenchmark, psych, update = FALSE)
# ===
===================
load_raster <- function(file_name){
# need to set the correct directory for this to work
file_name <- file_name
climate <- raster::raster(file_name)
climate
}
create_cropped_raster <- function(pop_raster, filtered_geo_shapefile) {
cropped_pop_raster <- raster::crop(pop_raster, filtered_geo_shapefile)
cropped_pop_raster <- raster::mask(cropped_pop_raster, filtered_geo_shapefile)
return(cropped_pop_raster)
}
add_mean_to_shapefile <- function(pop_raster, geo_area, year, month) {
clim <- raster::extract(pop_raster, geo_area, fun = mean, na.rm = TRUE, cellnumbers = T)
geo_area$mean_spi <- clim
geo_area$mean_spi <- as.numeric(geo_area$mean_spi)
geo_area$year <- year
geo_area$month <- month
return(geo_area)
}
process_shapefile  <- function(filename){
climate <- load_raster(filename)
cropped_climate <- create_cropped_raster(climate, aggshape)
year = str_sub(filename, 12,15)
month = as.numeric(str_sub(filename, 17,18))
climate_agg <- add_mean_to_shapefile(cropped_climate, aggshape, year, month)
return(climate_agg)
}
shapefile_to_df <- function(shapef){
df <- data.frame(shapef$year, shapef$month, shapef$admin1Name, shapef$admin2Name, shapef$mean_spi)
return(df)
}
# =================
# SETUP
# ================
# SOM - ADMIN LEVEL 2 - DISTRICT
area_file <- "C:/Users/Sev/Documents/GAM_SAM prediction/South Sudan/Data/Admin2_Shapefile/ssd_admbnda_adm2_Abyei_imwg_nbs_20180817.shp"
file.exists(area_file)
aggshape <- read_sf(area_file)
# ===================
# RUN
# ===================
# CREATE CSV
# raw_data_dir <- "J:/Mortality_estimation_crises/Nigeria/New Data Structure/Predictor data/As received/not_cleaned/Rainfall 2017-2018/"
raw_data_dir <- "C:/Users/Sev/Documents/GAM_SAM prediction/South Sudan/Data/Climate/Ssd_Climate Engine/"
setwd(raw_data_dir)
files <- list.files()
file_names <- files %>% str_subset(".tif")
View(aggshape)
# aggregate rasterized spi by shape for all files (returns a list of shapefiles)
agg_shapes <- lapply(file_names, process_shapefile)
# convert shapefils to df
clim_dfs <- lapply(agg_shapes, shapefile_to_df)
# combine all dfs
combined_df <-  do.call("rbind", clim_dfs)
names(combined_df)
# format combined dfs for csv output
names(combined_df) <- c("y", "m", "state", "county", "mean_spi")
head(combined_df)
combined_df <- select(combined_df, c("y", "m", "state", "county", "mean_spi"))
combined_df <- as.data.frame(combined_df)
# Save Csv
filename <- paste0(raw_data_dir,  "/SPI_Chirps_2015-2018_Standardized_From_1981_2019.csv")
write.csv(combined_df, filename)
#area_file <- "C:/Users/Sev/Documents/GAM_SAM prediction/South Sudan/Data/Admin2_Shapefile/ssd_admbnda_adm2_Abyei_imwg_nbs_20180817.shp"
area_file <- "C:/Users/Sev/Documents/GAM_SAM prediction/South Sudan/Data/Admin2_Shapefile/ssd_admbndl_admALL_imwg_nbs_itos_20180817.shp"
View(agg_shapes)
file.exists(area_file)
aggshape <- read_sf(area_file)
# ===================
# RUN
# ===================
# CREATE CSV
# raw_data_dir <- "J:/Mortality_estimation_crises/Nigeria/New Data Structure/Predictor data/As received/not_cleaned/Rainfall 2017-2018/"
raw_data_dir <- "C:/Users/Sev/Documents/GAM_SAM prediction/South Sudan/Data/Climate/Ssd_Climate Engine/"
setwd(raw_data_dir)
files <- list.files()
file_names <- files %>% str_subset(".tif")
# aggregate rasterized spi by shape for all files (returns a list of shapefiles)
agg_shapes <- lapply(file_names, process_shapefile)
# convert shapefils to df
clim_dfs <- lapply(agg_shapes, shapefile_to_df)
# combine all dfs
combined_df <-  do.call("rbind", clim_dfs)
names(combined_df)
# format combined dfs for csv output
names(combined_df) <- c("y", "m", "state", "county", "mean_spi")
head(combined_df)
combined_df <- select(combined_df, c("y", "m", "state", "county", "mean_spi"))
combined_df <- as.data.frame(combined_df)
View(combined_df)
#area_file <- "C:/Users/Sev/Documents/GAM_SAM prediction/South Sudan/Data/Admin2_Shapefile/ssd_admbnda_adm2_Abyei_imwg_nbs_20180817.shp"
area_file <- "C:/Users/Sev/Documents/GAM_SAM prediction/South Sudan/Data/Admin2_Shapefile/ssd_admbnda_adm2_imwg_nbs_20180817.shp"
file.exists(area_file)
#area_file <- "C:/Users/Sev/Documents/GAM_SAM prediction/South Sudan/Data/Admin2_Shapefile/ssd_admbnda_adm2_Abyei_imwg_nbs_20180817.shp"
area_file <- "C:/Users/Sev/Documents/GAM_SAM prediction/South Sudan/Data/Admin2_Shapefile/ssd_admbnda_adm2_imwg_nbs_20180817.shp"
file.exists(area_file)
aggshape <- read_sf(area_file)
View(aggshape)
# ===================
# CREATE CSV
# raw_data_dir <- "J:/Mortality_estimation_crises/Nigeria/New Data Structure/Predictor data/As received/not_cleaned/Rainfall 2017-2018/"
raw_data_dir <- "C:/Users/Sev/Documents/GAM_SAM prediction/South Sudan/Data/Climate/Ssd_Climate Engine/"
setwd(raw_data_dir)
files <- list.files()
file_names <- files %>% str_subset(".tif")
# aggregate rasterized spi by shape for all files (returns a list of shapefiles)
agg_shapes <- lapply(file_names, process_shapefile)
# convert shapefils to df
clim_dfs <- lapply(agg_shapes, shapefile_to_df)
# combine all dfs
combined_df <-  do.call("rbind", clim_dfs)
names(combined_df)
# format combined dfs for csv output
names(combined_df) <- c("y", "m", "state", "county", "mean_spi")
head(combined_df)
combined_df <- select(combined_df, c("y", "m", "state", "county", "mean_spi"))
combined_df <- as.data.frame(combined_df)
# Save Csv
filename <- paste0(raw_data_dir,  "/SPI_Chirps_2015-2018_Standardized_From_1981_2019.csv")
write.csv(combined_df, filename)
View(aggshape)
source('~/GAM_SAM prediction/South Sudan/Data/Climate/climate-data-shaping-SSD.r', echo=TRUE)
source('~/GAM_SAM prediction/South Sudan/Data/Climate/climate-data-shaping-SSD.r', echo=TRUE)
